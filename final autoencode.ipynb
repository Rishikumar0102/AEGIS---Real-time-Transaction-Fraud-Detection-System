{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4bef78-ee82-4255-ad6e-18e06ca0c212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ðŸ¤– CREDIT CARD FRAUD AUTOENCODER TRAINING (Version 002)\n",
      "======================================================================\n",
      "\n",
      "ðŸ“Š Loading dataset...\n",
      "Dataset: (100000, 17)\n",
      "Fraud rate: 13.46%\n",
      "Columns: ['transaction_id', 'cc_number', 'user_id', 'credit_limit', 'amount', 'transaction_type', 'city', 'card_lat', 'card_lon', 'merchant_lat', 'merchant_lon', 'merchant_name', 'timestamp', 'hour_of_day', 'day_of_week', 'is_night', 'is_fraud']\n",
      "\n",
      "ðŸ”§ Preparing features...\n",
      "Using 15 features: ['amount_to_limit', 'amount_log', 'is_large_amount', 'hour_bin', 'hour_sin', 'hour_cos', 'day_sin', 'day_cos', 'merchant_distance_km', 'distance_log', 'is_night', 'is_high_risk_time', 'txn_ATM', 'txn_ONLINE', 'txn_POS']\n",
      "\n",
      "ðŸ“ˆ Data split:\n",
      "  Training: (80000, 15) (Fraud: 10765)\n",
      "  Testing:  (20000, 15) (Fraud: 2691)\n",
      "\n",
      "ðŸ—ï¸  Building autoencoder architecture...\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 15)]              0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 64)                1024      \n",
      "                                                                 \n",
      " batch_normalization_8 (Bat  (None, 64)                256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " batch_normalization_9 (Bat  (None, 32)                128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 16)                528       \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 32)                544       \n",
      "                                                                 \n",
      " batch_normalization_10 (Ba  (None, 32)                128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 64)                2112      \n",
      "                                                                 \n",
      " batch_normalization_11 (Ba  (None, 64)                256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 15)                975       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8031 (31.37 KB)\n",
      "Trainable params: 7647 (29.87 KB)\n",
      "Non-trainable params: 384 (1.50 KB)\n",
      "_________________________________________________________________\n",
      "\n",
      "ðŸš€ Training autoencoder (using only normal transactions)...\n",
      "Training on 69235 normal transactions\n",
      "\n",
      "Starting training...\n",
      "Epoch 1/50\n",
      "460/460 [==============================] - 14s 14ms/step - loss: 1.3681 - mae: 0.8832 - val_loss: 0.4685 - val_mae: 0.4966 - lr: 5.0000e-04\n",
      "Epoch 2/50\n",
      "460/460 [==============================] - 5s 11ms/step - loss: 0.6604 - mae: 0.6091 - val_loss: 0.3453 - val_mae: 0.3988 - lr: 5.0000e-04\n",
      "Epoch 3/50\n",
      "460/460 [==============================] - 5s 11ms/step - loss: 0.5050 - mae: 0.5189 - val_loss: 0.2929 - val_mae: 0.3531 - lr: 5.0000e-04\n",
      "Epoch 4/50\n",
      "460/460 [==============================] - 5s 11ms/step - loss: 0.4403 - mae: 0.4785 - val_loss: 0.2567 - val_mae: 0.3245 - lr: 5.0000e-04\n",
      "Epoch 5/50\n",
      "460/460 [==============================] - 4s 10ms/step - loss: 0.3987 - mae: 0.4540 - val_loss: 0.2271 - val_mae: 0.3014 - lr: 5.0000e-04\n",
      "Epoch 6/50\n",
      "460/460 [==============================] - 5s 11ms/step - loss: 0.3668 - mae: 0.4359 - val_loss: 0.2014 - val_mae: 0.2847 - lr: 5.0000e-04\n",
      "Epoch 7/50\n",
      "460/460 [==============================] - 5s 12ms/step - loss: 0.3401 - mae: 0.4218 - val_loss: 0.1783 - val_mae: 0.2695 - lr: 5.0000e-04\n",
      "Epoch 8/50\n",
      "460/460 [==============================] - 6s 12ms/step - loss: 0.3118 - mae: 0.4060 - val_loss: 0.1355 - val_mae: 0.2443 - lr: 5.0000e-04\n",
      "Epoch 9/50\n",
      "460/460 [==============================] - 5s 10ms/step - loss: 0.2833 - mae: 0.3900 - val_loss: 0.1100 - val_mae: 0.2225 - lr: 5.0000e-04\n",
      "Epoch 10/50\n",
      "460/460 [==============================] - 5s 12ms/step - loss: 0.2598 - mae: 0.3752 - val_loss: 0.0943 - val_mae: 0.2084 - lr: 5.0000e-04\n",
      "Epoch 11/50\n",
      "460/460 [==============================] - 5s 11ms/step - loss: 0.2437 - mae: 0.3650 - val_loss: 0.0824 - val_mae: 0.1974 - lr: 5.0000e-04\n",
      "Epoch 12/50\n",
      "460/460 [==============================] - 4s 10ms/step - loss: 0.2313 - mae: 0.3567 - val_loss: 0.0723 - val_mae: 0.1849 - lr: 5.0000e-04\n",
      "Epoch 13/50\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 0.2205 - mae: 0.3486 - val_loss: 0.0684 - val_mae: 0.1808 - lr: 5.0000e-04\n",
      "Epoch 14/50\n",
      "460/460 [==============================] - 4s 10ms/step - loss: 0.2132 - mae: 0.3426 - val_loss: 0.0623 - val_mae: 0.1727 - lr: 5.0000e-04\n",
      "Epoch 15/50\n",
      "460/460 [==============================] - 5s 11ms/step - loss: 0.2047 - mae: 0.3359 - val_loss: 0.0536 - val_mae: 0.1601 - lr: 5.0000e-04\n",
      "Epoch 16/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.1990 - mae: 0.3308 - val_loss: 0.0508 - val_mae: 0.1575 - lr: 5.0000e-04\n",
      "Epoch 17/50\n",
      "460/460 [==============================] - 5s 10ms/step - loss: 0.1944 - mae: 0.3261 - val_loss: 0.0460 - val_mae: 0.1482 - lr: 5.0000e-04\n",
      "Epoch 18/50\n",
      "460/460 [==============================] - 5s 11ms/step - loss: 0.1885 - mae: 0.3214 - val_loss: 0.0422 - val_mae: 0.1430 - lr: 5.0000e-04\n",
      "Epoch 19/50\n",
      "460/460 [==============================] - 4s 10ms/step - loss: 0.1845 - mae: 0.3171 - val_loss: 0.0412 - val_mae: 0.1402 - lr: 5.0000e-04\n",
      "Epoch 20/50\n",
      "460/460 [==============================] - 5s 11ms/step - loss: 0.1818 - mae: 0.3147 - val_loss: 0.0398 - val_mae: 0.1397 - lr: 5.0000e-04\n",
      "Epoch 21/50\n",
      "460/460 [==============================] - 5s 12ms/step - loss: 0.1794 - mae: 0.3117 - val_loss: 0.0370 - val_mae: 0.1319 - lr: 5.0000e-04\n",
      "Epoch 22/50\n",
      "460/460 [==============================] - 5s 12ms/step - loss: 0.1765 - mae: 0.3092 - val_loss: 0.0379 - val_mae: 0.1352 - lr: 5.0000e-04\n",
      "Epoch 23/50\n",
      "460/460 [==============================] - 5s 11ms/step - loss: 0.1735 - mae: 0.3060 - val_loss: 0.0357 - val_mae: 0.1297 - lr: 5.0000e-04\n",
      "Epoch 24/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.1713 - mae: 0.3035 - val_loss: 0.0354 - val_mae: 0.1308 - lr: 5.0000e-04\n",
      "Epoch 25/50\n",
      "460/460 [==============================] - 6s 14ms/step - loss: 0.1695 - mae: 0.3014 - val_loss: 0.0339 - val_mae: 0.1278 - lr: 5.0000e-04\n",
      "Epoch 26/50\n",
      "460/460 [==============================] - 6s 14ms/step - loss: 0.1674 - mae: 0.2991 - val_loss: 0.0329 - val_mae: 0.1250 - lr: 5.0000e-04\n",
      "Epoch 27/50\n",
      "460/460 [==============================] - 6s 14ms/step - loss: 0.1655 - mae: 0.2966 - val_loss: 0.0317 - val_mae: 0.1244 - lr: 5.0000e-04\n",
      "Epoch 28/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.1640 - mae: 0.2951 - val_loss: 0.0335 - val_mae: 0.1290 - lr: 5.0000e-04\n",
      "Epoch 29/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.1617 - mae: 0.2928 - val_loss: 0.0309 - val_mae: 0.1207 - lr: 5.0000e-04\n",
      "Epoch 30/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.1612 - mae: 0.2922 - val_loss: 0.0301 - val_mae: 0.1181 - lr: 5.0000e-04\n",
      "Epoch 31/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.1602 - mae: 0.2909 - val_loss: 0.0309 - val_mae: 0.1210 - lr: 5.0000e-04\n",
      "Epoch 32/50\n",
      "460/460 [==============================] - 6s 14ms/step - loss: 0.1597 - mae: 0.2899 - val_loss: 0.0310 - val_mae: 0.1214 - lr: 5.0000e-04\n",
      "Epoch 33/50\n",
      "460/460 [==============================] - 7s 14ms/step - loss: 0.1579 - mae: 0.2881 - val_loss: 0.0300 - val_mae: 0.1182 - lr: 5.0000e-04\n",
      "Epoch 34/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.1581 - mae: 0.2879 - val_loss: 0.0317 - val_mae: 0.1254 - lr: 5.0000e-04\n",
      "Epoch 35/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.1566 - mae: 0.2865 - val_loss: 0.0308 - val_mae: 0.1214 - lr: 5.0000e-04\n",
      "Epoch 36/50\n",
      "460/460 [==============================] - 6s 12ms/step - loss: 0.1564 - mae: 0.2862 - val_loss: 0.0305 - val_mae: 0.1225 - lr: 5.0000e-04\n",
      "Epoch 37/50\n",
      "460/460 [==============================] - 6s 12ms/step - loss: 0.1559 - mae: 0.2855 - val_loss: 0.0303 - val_mae: 0.1198 - lr: 5.0000e-04\n",
      "Epoch 38/50\n",
      "456/460 [============================>.] - ETA: 0s - loss: 0.1553 - mae: 0.2845\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "460/460 [==============================] - 6s 12ms/step - loss: 0.1553 - mae: 0.2845 - val_loss: 0.0323 - val_mae: 0.1263 - lr: 5.0000e-04\n",
      "Epoch 39/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.1530 - mae: 0.2827 - val_loss: 0.0292 - val_mae: 0.1176 - lr: 2.5000e-04\n",
      "Epoch 40/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.1535 - mae: 0.2828 - val_loss: 0.0288 - val_mae: 0.1177 - lr: 2.5000e-04\n",
      "Epoch 41/50\n",
      "460/460 [==============================] - 6s 12ms/step - loss: 0.1520 - mae: 0.2816 - val_loss: 0.0278 - val_mae: 0.1148 - lr: 2.5000e-04\n",
      "Epoch 42/50\n",
      "460/460 [==============================] - 6s 14ms/step - loss: 0.1512 - mae: 0.2806 - val_loss: 0.0283 - val_mae: 0.1161 - lr: 2.5000e-04\n",
      "Epoch 43/50\n",
      "460/460 [==============================] - 6s 12ms/step - loss: 0.1516 - mae: 0.2813 - val_loss: 0.0288 - val_mae: 0.1181 - lr: 2.5000e-04\n",
      "Epoch 44/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.1503 - mae: 0.2801 - val_loss: 0.0277 - val_mae: 0.1131 - lr: 2.5000e-04\n",
      "Epoch 45/50\n",
      "460/460 [==============================] - 6s 12ms/step - loss: 0.1511 - mae: 0.2805 - val_loss: 0.0276 - val_mae: 0.1139 - lr: 2.5000e-04\n",
      "Epoch 46/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.1509 - mae: 0.2808 - val_loss: 0.0291 - val_mae: 0.1184 - lr: 2.5000e-04\n",
      "Epoch 47/50\n",
      "460/460 [==============================] - 7s 15ms/step - loss: 0.1505 - mae: 0.2801 - val_loss: 0.0283 - val_mae: 0.1167 - lr: 2.5000e-04\n",
      "Epoch 48/50\n",
      "460/460 [==============================] - 7s 15ms/step - loss: 0.1498 - mae: 0.2794 - val_loss: 0.0279 - val_mae: 0.1162 - lr: 2.5000e-04\n",
      "Epoch 49/50\n",
      "460/460 [==============================] - 7s 15ms/step - loss: 0.1493 - mae: 0.2792 - val_loss: 0.0275 - val_mae: 0.1145 - lr: 2.5000e-04\n",
      "Epoch 50/50\n",
      "460/460 [==============================] - 7s 16ms/step - loss: 0.1485 - mae: 0.2783 - val_loss: 0.0270 - val_mae: 0.1133 - lr: 2.5000e-04\n",
      "âœ… Training completed with best weights restored automatically\n",
      "Using default threshold (97th percentile): 0.0777\n",
      "\n",
      "ðŸ“Š Evaluating model performance...\n",
      "\n",
      "============================================================\n",
      "BEST THRESHOLD: 90.0th percentile\n",
      "Threshold value: 0.0971\n",
      "============================================================\n",
      "\n",
      "Accuracy:  0.9308\n",
      "Precision: 0.8270\n",
      "Recall:    0.6146\n",
      "F1-Score:  0.7052\n",
      "Fraud Detected: 2000.0 / 20000\n",
      "\n",
      "ðŸ’¾ Saving trained model (with 002 suffix)...\n",
      "\n",
      "ðŸ’¾ Saving model files as fraud_detection_model_002...\n",
      "âœ… Saved Keras format: fraud_detection_model_002.keras\n",
      "âœ… Saved H5 format: fraud_detection_model_002.h5\n",
      "âœ… Saved scaler: fraud_detection_model_002_scaler.pkl\n",
      "âœ… Saved threshold: fraud_detection_model_002_threshold.pkl (value: 0.0971)\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 15)]              0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 64)                1024      \n",
      "                                                                 \n",
      " batch_normalization_8 (Bat  (None, 64)                256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " batch_normalization_9 (Bat  (None, 32)                128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 16)                528       \n",
      "                                                                 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rishi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dense_15 (Dense)            (None, 32)                544       \n",
      "                                                                 \n",
      " batch_normalization_10 (Ba  (None, 32)                128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 64)                2112      \n",
      "                                                                 \n",
      " batch_normalization_11 (Ba  (None, 64)                256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 15)                975       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8031 (31.37 KB)\n",
      "Trainable params: 7647 (29.87 KB)\n",
      "Non-trainable params: 384 (1.50 KB)\n",
      "_________________________________________________________________\n",
      "âœ… Saved config: fraud_detection_model_002_config.json\n",
      "âœ… Saved architecture: fraud_detection_model_002_architecture.json\n",
      "âœ… Saved weights: fraud_detection_model_002.weights.h5\n",
      "\n",
      "ðŸŽ¯ Model saved in 7 formats as fraud_detection_model_002! Ready for deployment.\n",
      "\n",
      "ðŸ§ª Quick test with sample data...\n",
      "Sample MSE: 0.0201\n",
      "Threshold: 0.0971\n",
      "Is Fraud? NO âœ…\n",
      "\n",
      "======================================================================\n",
      "ðŸŽ‰ TRAINING COMPLETE! (Version 002)\n",
      "======================================================================\n",
      "\n",
      "ðŸ“ Files created (with 002 suffix):\n",
      "  1. fraud_detection_model_002.keras  (Recommended for FastAPI)\n",
      "  2. fraud_detection_model_002.h5     (Legacy format)\n",
      "  3. fraud_detection_model_002_scaler.pkl\n",
      "  4. fraud_detection_model_002_threshold.pkl\n",
      "  5. fraud_detection_model_002_config.json\n",
      "  6. fraud_detection_model_002_architecture.json\n",
      "  7. fraud_detection_model_002.weights.h5\n",
      "\n",
      "ðŸ“Š Model Performance (002):\n",
      "  F1-Score: 0.7052\n",
      "  Accuracy: 0.9308\n",
      "  Precision: 0.8270\n",
      "  Recall: 0.6146\n",
      "\n",
      "ðŸš€ Ready for deployment with FastAPI!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model\n",
    "import joblib\n",
    "import json\n",
    "import os\n",
    "\n",
    "class FraudAutoencoderKeras:\n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.model = None\n",
    "        self.threshold = 0.0\n",
    "        self.feature_columns = []\n",
    "        \n",
    "    def prepare_features(self, df):\n",
    "        \"\"\"Create better features for fraud detection\"\"\"\n",
    "        features = df.copy()\n",
    "        \n",
    "        \n",
    "        features['amount_to_limit'] = features['amount'] / features['credit_limit']\n",
    "        features['amount_log'] = np.log1p(features['amount'])\n",
    "        features['is_large_amount'] = (features['amount'] > features['credit_limit'] * 0.3).astype(float)\n",
    "        \n",
    "       \n",
    "        def get_hour_bin(hour):\n",
    "            if 6 <= hour < 12: return 0  # Morning\n",
    "            elif 12 <= hour < 18: return 1  # Afternoon\n",
    "            elif 18 <= hour < 22: return 2  # Evening\n",
    "            else: return 3  # Night\n",
    "        \n",
    "        features['hour_bin'] = features['hour_of_day'].apply(get_hour_bin)\n",
    "        features['hour_sin'] = np.sin(2 * np.pi * features['hour_of_day'] / 24)\n",
    "        features['hour_cos'] = np.cos(2 * np.pi * features['hour_of_day'] / 24)\n",
    "        features['day_sin'] = np.sin(2 * np.pi * features['day_of_week'] / 7)\n",
    "        features['day_cos'] = np.cos(2 * np.pi * features['day_of_week'] / 7)\n",
    "        \n",
    "        # improve the location and use the harversine \n",
    "        def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "            \"\"\" Haversine formula\"\"\"\n",
    "            R = 6371  # Earth radius in kilometers\n",
    "            \n",
    "            # Convert to radians\n",
    "            lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "            \n",
    "            # Haversine formula\n",
    "            dlat = lat2 - lat1\n",
    "            dlon = lon2 - lon1\n",
    "            a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "            c = 2 * np.arcsin(np.sqrt(a))\n",
    "            \n",
    "            return R * c\n",
    "        \n",
    "        features['merchant_distance_km'] = haversine_distance(\n",
    "            features['card_lat'].values,\n",
    "            features['card_lon'].values,\n",
    "            features['merchant_lat'].values,\n",
    "            features['merchant_lon'].values\n",
    "        )\n",
    "        features['distance_log'] = np.log1p(features['merchant_distance_km'])\n",
    "        \n",
    "        # 4. TRANSACTION TYPE FEATURES\n",
    "        # One-hot encode transaction types (ATM more risky)\n",
    "        transaction_dummies = pd.get_dummies(features['transaction_type'], prefix='txn')\n",
    "        features = pd.concat([features, transaction_dummies], axis=1)\n",
    "        \n",
    "        # 5. RISK TIME FEATURES\n",
    "        features['is_high_risk_time'] = ((features['is_night'] == 1) | \n",
    "                                        (features['hour_of_day'] < 6) |\n",
    "                                        (features['hour_of_day'] > 22)).astype(float)\n",
    "        \n",
    "        # Select final features\n",
    "        feature_cols = [\n",
    "            'amount_to_limit', 'amount_log', 'is_large_amount',\n",
    "            'hour_bin', 'hour_sin', 'hour_cos', 'day_sin', 'day_cos',\n",
    "            'merchant_distance_km', 'distance_log',\n",
    "            'is_night', 'is_high_risk_time'\n",
    "        ]\n",
    "        \n",
    "        # Add transaction type columns\n",
    "        txn_cols = [col for col in features.columns if col.startswith('txn_')]\n",
    "        feature_cols.extend(txn_cols)\n",
    "        \n",
    "        # Keep only columns that exist\n",
    "        available_cols = [col for col in feature_cols if col in features.columns]\n",
    "        self.feature_columns = available_cols\n",
    "        \n",
    "        X = features[available_cols].fillna(0)\n",
    "        y = features['is_fraud']\n",
    "        \n",
    "        print(f\"Using {len(available_cols)} features: {available_cols}\")\n",
    "        return X, y\n",
    "    \n",
    "    def build_improved_model(self, input_dim):\n",
    "        \"\"\"Build better autoencoder architecture\"\"\"\n",
    "        input_layer = layers.Input(shape=(input_dim,))\n",
    "        \n",
    "        # Encoder - deeper architecture\n",
    "        x = layers.Dense(64, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001))(input_layer)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(0.3)(x)\n",
    "        \n",
    "        x = layers.Dense(32, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001))(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(0.3)(x)\n",
    "        \n",
    "        # Bottleneck\n",
    "        encoded = layers.Dense(16, activation='relu')(x)\n",
    "        \n",
    "        # Decoder\n",
    "        x = layers.Dense(32, activation='relu')(encoded)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(0.3)(x)\n",
    "        \n",
    "        x = layers.Dense(64, activation='relu')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(0.3)(x)\n",
    "        \n",
    "        decoded = layers.Dense(input_dim, activation='linear')(x)\n",
    "        \n",
    "        self.model = Model(input_layer, decoded)\n",
    "        \n",
    "        # Compile with better optimizer\n",
    "        self.model.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=0.0005),\n",
    "            loss='mse',\n",
    "            metrics=['mae']\n",
    "        )\n",
    "        \n",
    "        return self.model\n",
    "    \n",
    "    def train_with_validation(self, X_train, y_train, epochs=50, batch_size=128):\n",
    "        \"\"\"Train with early stopping and validation - NO ModelCheckpoint\"\"\"\n",
    "        # Scale features\n",
    "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
    "        \n",
    "        # Use only normal transactions\n",
    "        X_train_normal = X_train_scaled[y_train == 0]\n",
    "        \n",
    "        print(f\"Training on {len(X_train_normal)} normal transactions\")\n",
    "        \n",
    "        # Train/validation split\n",
    "        X_train_split, X_val_split = train_test_split(\n",
    "            X_train_normal, test_size=0.15, random_state=42\n",
    "        )\n",
    "        \n",
    "        \n",
    "        callbacks = [\n",
    "            keras.callbacks.EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=10,\n",
    "                restore_best_weights=True,  \n",
    "                verbose=1\n",
    "            ),\n",
    "            keras.callbacks.ReduceLROnPlateau(\n",
    "                monitor='val_loss',\n",
    "                factor=0.5,\n",
    "                patience=5,\n",
    "                min_lr=0.00001,\n",
    "                verbose=1\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        # Train\n",
    "        print(\"\\nStarting training...\")\n",
    "        history = self.model.fit(\n",
    "            X_train_split, X_train_split,\n",
    "            validation_data=(X_val_split, X_val_split),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        val_reconstructions = self.model.predict(X_val_split, verbose=0)\n",
    "        val_mse = np.mean(np.power(X_val_split - val_reconstructions, 2), axis=1)\n",
    "        for percentile in [90, 92, 94, 95, 96, 97, 98]:\n",
    "            threshold = np.percentile(val_mse, percentile)\n",
    "            preds = (val_mse > threshold).astype(int)\n",
    "            fp_rate = preds.mean()  \n",
    "            if fp_rate < 0.02:  \n",
    "                self.threshold = threshold\n",
    "                print(f\"Selected threshold at {percentile}th percentile: {threshold:.4f}\")\n",
    "                print(f\"False positive rate: {fp_rate:.4f}\")\n",
    "                break\n",
    "        \n",
    "        if self.threshold == 0:\n",
    "            self.threshold = np.percentile(val_mse, 97)  # Default\n",
    "            print(f\"Using default threshold (97th percentile): {self.threshold:.4f}\")\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    def evaluate_model(self, X_test, y_test):\n",
    "        X_test_scaled = self.scaler.transform(X_test)\n",
    "        reconstructions = self.model.predict(X_test_scaled, verbose=0)\n",
    "        mse_scores = np.mean(np.power(X_test_scaled - reconstructions, 2), axis=1)\n",
    "        results = []\n",
    "        for percentile in [90, 92, 94, 95, 96, 97, 98]:\n",
    "            threshold = np.percentile(mse_scores, percentile)\n",
    "            y_pred = (mse_scores > threshold).astype(int)\n",
    "            cm = confusion_matrix(y_test, y_pred)\n",
    "            tn, fp, fn, tp = cm.ravel()\n",
    "            accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "            \n",
    "            results.append({\n",
    "                'percentile': percentile,\n",
    "                'threshold': threshold,\n",
    "                'accuracy': accuracy,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1': f1,\n",
    "                'detected': y_pred.sum()\n",
    "            })\n",
    "        results_df = pd.DataFrame(results)\n",
    "        best_idx = results_df['f1'].idxmax()\n",
    "        best_result = results_df.loc[best_idx]\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"BEST THRESHOLD: {best_result['percentile']}th percentile\")\n",
    "        print(f\"Threshold value: {best_result['threshold']:.4f}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        print(f\"\\nAccuracy:  {best_result['accuracy']:.4f}\")\n",
    "        print(f\"Precision: {best_result['precision']:.4f}\")\n",
    "        print(f\"Recall:    {best_result['recall']:.4f}\")\n",
    "        print(f\"F1-Score:  {best_result['f1']:.4f}\")\n",
    "        print(f\"Fraud Detected: {best_result['detected']} / {len(y_test)}\")\n",
    "        self.threshold = best_result['threshold']\n",
    "        \n",
    "        return best_result\n",
    "    \n",
    "    def save_model(self, base_name='fraud_autoencoder'):\n",
    "        base_name_with_suffix = f\"{base_name}_002\"\n",
    "        keras_path = f'{base_name_with_suffix}.keras'\n",
    "        self.model.save(keras_path)\n",
    "        print(f\" Saved Keras format: {keras_path}\")\n",
    "        h5_path = f'{base_name_with_suffix}.h5'\n",
    "        self.model.save(h5_path)\n",
    "        print(f\"Saved H5 format: {h5_path}\")\n",
    "        scaler_path = f'{base_name_with_suffix}_scaler.pkl'\n",
    "        joblib.dump(self.scaler, scaler_path)\n",
    "        print(f\"Saved scaler: {scaler_path}\")\n",
    "        threshold_path = f'{base_name_with_suffix}_threshold.pkl'\n",
    "        joblib.dump(self.threshold, threshold_path)\n",
    "        print(f\"Saved threshold: {threshold_path} (value: {self.threshold:.4f})\")\n",
    "        config = {\n",
    "            'feature_columns': self.feature_columns,\n",
    "            'input_dim': len(self.feature_columns),\n",
    "            'threshold': float(self.threshold),\n",
    "            'model_summary': str(self.model.summary()),\n",
    "            'training_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'version': '002'\n",
    "        }\n",
    "        \n",
    "        config_path = f'{base_name_with_suffix}_config.json'\n",
    "        with open(config_path, 'w') as f:\n",
    "            json.dump(config, f, indent=2)\n",
    "        print(f\"Saved config: {config_path}\")\n",
    "        model_json = self.model.to_json()\n",
    "        with open(f'{base_name_with_suffix}_architecture.json', 'w') as f:\n",
    "            f.write(model_json)\n",
    "        print(f\"Saved architecture: {base_name_with_suffix}_architecture.json\")\n",
    "        weights_path = f'{base_name_with_suffix}.weights.h5'\n",
    "        self.model.save_weights(weights_path)\n",
    "        print(f\"âœ… Saved weights: {weights_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\"*70)\n",
    "    print(\"Traning)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    \n",
    "    df = pd.read_csv('current.csv')\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    \n",
    "    print(f\"Dataset: {df.shape}\")\n",
    "    print(f\"Fraud rate: {df['is_fraud'].mean()*100:.2f}%\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    \n",
    "    # Initialize autoencoder\n",
    "    autoencoder = FraudAutoencoderKeras()\n",
    "    \n",
    "    \n",
    "    print(\"\\nðŸ”§ Preparing features...\")\n",
    "    X, y = autoencoder.prepare_features(df)\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, stratify=y, random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nðŸ“ˆ Data split:\")\n",
    "    print(f\"  Training: {X_train.shape} (Fraud: {y_train.sum()})\")\n",
    "    print(f\"  Testing:  {X_test.shape} (Fraud: {y_test.sum()})\")\n",
    "    \n",
    "    autoencoder.build_improved_model(input_dim=X_train.shape[1])\n",
    "    autoencoder.model.summary()\n",
    "    \n",
    "   \n",
    "    history = autoencoder.train_with_validation(X_train, y_train, epochs=50)\n",
    "    \n",
    "    \n",
    "    print(\"\\nðŸ“Š Evaluating model performance...\")\n",
    "    best_result = autoencoder.evaluate_model(X_test, y_test)\n",
    "    \n",
    "    print(\"\\nðŸ’¾ Saving trained model (with 002 suffix)...\")\n",
    "    autoencoder.save_model('fraud_detection_model')\n",
    "    \n",
    "  \n",
    "    sample_idx = np.random.randint(0, len(X_test))\n",
    "    sample = X_test.iloc[[sample_idx]]\n",
    "    sample_scaled = autoencoder.scaler.transform(sample)\n",
    "    reconstruction = autoencoder.model.predict(sample_scaled, verbose=0)\n",
    "    mse = np.mean(np.power(sample_scaled - reconstruction, 2))\n",
    "    \n",
    "    print(f\"Sample MSE: {mse:.4f}\")\n",
    "    print(f\"Threshold: {autoencoder.threshold:.4f}\")\n",
    "    print(f\"Is Fraud? {'YES ðŸš¨' if mse > autoencoder.threshold else 'NO âœ…'}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"=\"*70)\n",
    "   \n",
    "    print(\"  1. fraud_detection_model_002.keras  (Recommended for FastAPI)\")\n",
    "    print(\"  2. fraud_detection_model_002.h5     (Legacy format)\")\n",
    "    print(\"  3. fraud_detection_model_002_scaler.pkl\")\n",
    "    print(\"  4. fraud_detection_model_002_threshold.pkl\")\n",
    "    print(\"  5. fraud_detection_model_002_config.json\")\n",
    "    print(\"  6. fraud_detection_model_002_architecture.json\")\n",
    "    print(\"  7. fraud_detection_model_002.weights.h5\")\n",
    "    print(\"\\nðŸ“Š Model Performance (002):\")\n",
    "    print(f\"  F1-Score: {best_result['f1']:.4f}\")\n",
    "    print(f\"  Accuracy: {best_result['accuracy']:.4f}\")\n",
    "    print(f\"  Precision: {best_result['precision']:.4f}\")\n",
    "    print(f\"  Recall: {best_result['recall']:.4f}\")\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47a79e6-f4b3-42b0-a5a9-4290985e8745",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
